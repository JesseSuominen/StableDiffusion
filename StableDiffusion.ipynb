{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JesseSuominen/StableDiffusion/blob/main/StableDiffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJWFTB8aVxy-"
      },
      "source": [
        "# Stable Diffusion Initialisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbmk3JqVs5yQ"
      },
      "outputs": [],
      "source": [
        "!pip install transformers diffusers lpips accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nl3JP8iYUTVY"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtV9OsG2UkUD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler\n",
        "from tqdm.auto import tqdm\n",
        "from torch import autocast\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy\n",
        "from torchvision import transforms as tfms\n",
        "\n",
        "# For video display:\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "# Set device\n",
        "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t96p567IUqzB"
      },
      "outputs": [],
      "source": [
        "# Load the autoencoder model which will be used to decode the latents into image space.\n",
        "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", use_auth_token=True)\n",
        "\n",
        "# Load the tokenizer and text encoder to tokenize and encode the text.\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "\n",
        "# The UNet model for generating the latents.\n",
        "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", use_auth_token=True)\n",
        "\n",
        "# The noise scheduler\n",
        "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
        "\n",
        "# GPU init\n",
        "vae = vae.to(torch_device)\n",
        "text_encoder = text_encoder.to(torch_device)\n",
        "unet = unet.to(torch_device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQM7XgxYV6hg"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect to Google Drive\n"
      ],
      "metadata": {
        "id": "d27QsxFZnLFe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYoviVgUXjwZ"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Single image creation"
      ],
      "metadata": {
        "id": "oqqU0y7W9XZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#prompt = [\"Nature, market, flora, fauna, noon, trading , 4k, detailed, trending in artstation, fantasy, vivid colors\"]\n",
        "#prompt = [\"A digital illustration of a steampunk computer laboratory with clockwork machines, 4k, detailed, trending in artstation, fantasy vivid colors\"]\n",
        "#prompt = [\"Shrek with hes friends in a park sipping some tea, 4k, detailed, vivid colors\"]\n",
        "prompt = [\"A painting of a futuristic re-search lab with computers and holorgrams, 4k, detailed, trending in artstation\"]\n",
        "#prompt = [\"A teddy bear in a room with lots of toys, 4k, detailed, trending in artstation\"]\n",
        "\n",
        "height = 512\n",
        "width = 768\n",
        "num_inference_steps = 100\n",
        "guidance_scale = 8\n",
        "generator = torch.manual_seed(42) #steampunk seed (4)\n",
        "batch_size = 1\n",
        "\n",
        "# Prep text\n",
        "text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "  text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
        "max_length = text_input.input_ids.shape[-1]\n",
        "uncond_input = tokenizer(\n",
        "    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        ")\n",
        "with torch.no_grad():\n",
        "  uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
        "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "# Prep Scheduler\n",
        "scheduler.set_timesteps(num_inference_steps)\n",
        "\n",
        "# Prep latents\n",
        "latents = torch.randn(\n",
        "(batch_size, unet.in_channels, height // 8, width // 8),\n",
        "generator=generator,\n",
        ")\n",
        "\n",
        "latents = latents.to(torch_device)\n",
        "latents = latents * scheduler.init_noise_sigma\n",
        "\n",
        "    # Loop\n",
        "with autocast(\"cuda\"):\n",
        "    for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
        "\n",
        "        latent_model_input = torch.cat([latents] * 2)\n",
        "        sigma = scheduler.sigmas[i]\n",
        "        latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
        "\n",
        "        # predict the noise residual\n",
        "        with torch.no_grad():\n",
        "            noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
        "\n",
        "        # perform guidance\n",
        "        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "        # compute the previous noisy sample x_t -> x_t-1\n",
        "        latents = scheduler.step(noise_pred, t, latents)[\"prev_sample\"]\n",
        "\n",
        "# scale and decode the image latents with vae\n",
        "latents = 1 / 0.18215 * latents\n",
        "\n",
        "with torch.no_grad():\n",
        "    image = vae.decode(latents).sample\n",
        "\n",
        "# Display\n",
        "image = (image / 2 + 0.5).clamp(0, 1)\n",
        "image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "images = (image * 255).round().astype(\"uint8\")\n",
        "pil_images = [Image.fromarray(image) for image in images]\n",
        "pil_images[0].save(f'/content/drive/MyDrive/stablediff/steampunk.jpeg')\n",
        "pil_images[0]"
      ],
      "metadata": {
        "id": "ntjZ-QbCxVSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Loop"
      ],
      "metadata": {
        "id": "KMcYtRltEmzb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etHajiDAUyMO"
      },
      "outputs": [],
      "source": [
        "# Make a folder to store results\n",
        "!rm -rf /content/drive/MyDrive/stablediff/NatureMarket\n",
        "!mkdir -p /content/drive/MyDrive/stablediff/CityImages\n",
        "\n",
        "# Some settings\n",
        "prompt = [\"City landscape from a close POV, 4k, detailed, trending in artstation, vivid colors\"]\n",
        "#prompt = [\"an electric sky city on an alien world\"]\n",
        "height = 512                        # default height of Stable Diffusion\n",
        "width = 768                         # default width of Stable Diffusion\n",
        "num_inference_steps = 200            # Number of denoising steps\n",
        "guidance_scale = 8.0                # Scale for classifier-free guidance\n",
        "generator = torch.manual_seed(23)   # Seed generator to create the inital latent noise\n",
        "batch_size = 1\n",
        "\n",
        "# Prep text\n",
        "text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "  text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
        "max_length = text_input.input_ids.shape[-1]\n",
        "uncond_input = tokenizer(\n",
        "    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        ")\n",
        "with torch.no_grad():\n",
        "  uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
        "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "for idx in range(5): #Number of images created\n",
        "    generator = torch.manual_seed(idx)\n",
        "\n",
        "    # Prep Scheduler\n",
        "    scheduler.set_timesteps(num_inference_steps)\n",
        "\n",
        "    # Prep latents\n",
        "    latents = torch.randn(\n",
        "    (batch_size, unet.in_channels, height // 8, width // 8),\n",
        "    generator=generator,\n",
        "    )\n",
        "    latents = latents.to(torch_device)\n",
        "    latents = latents * scheduler.init_noise_sigma # Need to scale to match k\n",
        "\n",
        "    # Loop\n",
        "    with autocast(\"cuda\"):\n",
        "        for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
        "            # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
        "            latent_model_input = torch.cat([latents] * 2)\n",
        "            sigma = scheduler.sigmas[i]\n",
        "            latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
        "\n",
        "            # predict the noise residual\n",
        "            with torch.no_grad():\n",
        "                noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
        "\n",
        "            # perform guidance\n",
        "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "            # compute the previous noisy sample x_t -> x_t-1\n",
        "            latents = scheduler.step(noise_pred, t, latents)[\"prev_sample\"]\n",
        "\n",
        "    # scale and decode the image latents with vae\n",
        "    latents = 1 / 0.18215 * latents\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image = vae.decode(latents).sample\n",
        "\n",
        "    # Display\n",
        "    image = (image / 2 + 0.5).clamp(0, 1)\n",
        "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "    images = (image * 255).round().astype(\"uint8\")\n",
        "    pil_images = [Image.fromarray(image) for image in images]\n",
        "    pil_images[0].save(f'/content/drive/MyDrive/stablediff/CityImages/CityImage{idx:04}.jpeg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oU-g0cqfWHGp"
      },
      "outputs": [],
      "source": [
        "# Using torchvision.transforms.ToTensor\n",
        "to_tensor_tfm = tfms.ToTensor()\n",
        "\n",
        "def pil_to_latent(input_im):\n",
        "  # Single image -> single latent in a batch (so size 1, 4, 64, 64)\n",
        "  with torch.no_grad():\n",
        "    latent = vae.encode(to_tensor_tfm(input_im).unsqueeze(0).to(torch_device)*2-1) # Note scaling\n",
        "  return 0.18215 * latent.latent_dist.mode() # or .mean or .sample\n",
        "\n",
        "def latents_to_pil(latents):\n",
        "  # bath of latents -> list of images\n",
        "  latents = (1 / 0.18215) * latents\n",
        "  with torch.no_grad():\n",
        "    image = vae.decode(latents)[0]\n",
        "  image = (image / 2 + 0.5).clamp(0, 1)\n",
        "  image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "  images = (image * 255).round().astype(\"uint8\")\n",
        "  pil_images = [Image.fromarray(image) for image in images]\n",
        "  return pil_images"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "im = Image.open('/content/drive/MyDrive/stablediff/Images/Teddybear.jpeg').convert('RGB')\n",
        "im = im.resize((512,512))\n",
        "encoded = pil_to_latent(im)\n",
        "im"
      ],
      "metadata": {
        "id": "0Lb7v2EfICDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mixed guidance"
      ],
      "metadata": {
        "id": "VYwsF4udLXY_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fB3o7-WZGo5l"
      },
      "outputs": [],
      "source": [
        "prompts = ['blue fire', 'reticulated python in a tree']\n",
        "weights = [0.5,0.5]\n",
        "height = 512\n",
        "width = 768\n",
        "num_inference_steps = 50\n",
        "guidance_scale = 8\n",
        "generator = torch.manual_seed(5)\n",
        "batch_size = 1\n",
        "\n",
        "# Prep text\n",
        "# Embed both prompts\n",
        "text_embeddings = []\n",
        "for i in range(len(prompts)):\n",
        "    text_input = tokenizer([prompts[i]], padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        text_embeddings.append(text_encoder(text_input.input_ids.to(torch_device))[0])\n",
        "\n",
        "# Take the average\n",
        "weighted_embeddings = torch.zeros(text_embeddings[0].shape).to(torch_device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(len(prompts)):\n",
        "        weighted_embeddings.add_(text_embeddings[i] * weights[i])\n",
        "\n",
        "text_embeddings = weighted_embeddings\n",
        "\n",
        "# And the uncond. input as before:\n",
        "max_length = text_input.input_ids.shape[-1]\n",
        "uncond_input = tokenizer(\n",
        "    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        ")\n",
        "with torch.no_grad():\n",
        "  uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
        "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "# Prep Scheduler\n",
        "scheduler.set_timesteps(num_inference_steps)\n",
        "\n",
        "# Prep latents\n",
        "latents = torch.randn(\n",
        "  (batch_size, unet.in_channels, height // 8, width // 8),\n",
        "  generator=generator,\n",
        ")\n",
        "latents = latents.to(torch_device)\n",
        "latents = latents * scheduler.sigmas[0] # Need to scale to match k\n",
        "\n",
        "# Loop\n",
        "with autocast(\"cuda\"):\n",
        "  for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
        "    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
        "    latent_model_input = torch.cat([latents] * 2)\n",
        "    sigma = scheduler.sigmas[i]\n",
        "    latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
        "\n",
        "    # predict the noise residual\n",
        "    with torch.no_grad():\n",
        "      noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
        "\n",
        "    # perform guidance\n",
        "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "    # compute the previous noisy sample x_t -> x_t-1\n",
        "    latents = scheduler.step(noise_pred, t, latents)[\"prev_sample\"]\n",
        "\n",
        "latents_to_pil(latents)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Denoising Example"
      ],
      "metadata": {
        "id": "QV4JlBIZqxvc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "By-8CPHZM_v8"
      },
      "outputs": [],
      "source": [
        "prompt = 'a futuristic city, abandoned and overgrown with plants, dystopia, bathed in sunlight'\n",
        "height = 512\n",
        "width = 768\n",
        "num_inference_steps = 100\n",
        "guidance_scale = 8\n",
        "generator = torch.manual_seed(10)\n",
        "batch_size = 1\n",
        "\n",
        "# Make a folder to store results\n",
        "!rm -rf /content/drive/MyDrive/stablediff/denoising2/\n",
        "!mkdir -p /content/drive/MyDrive/stablediff/denoising2/\n",
        "\n",
        "# Prep text\n",
        "text_input = tokenizer([prompt], padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "  text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
        "\n",
        "# the uncond. input\n",
        "max_length = text_input.input_ids.shape[-1]\n",
        "uncond_input = tokenizer(\n",
        "    [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
        ")\n",
        "with torch.no_grad():\n",
        "  uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
        "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "# Prep Scheduler\n",
        "scheduler.set_timesteps(num_inference_steps)\n",
        "\n",
        "# Prep latents\n",
        "latents = torch.randn(\n",
        "  (batch_size, unet.in_channels, height // 8, width // 8),\n",
        "  generator=generator,\n",
        ")\n",
        "latents = latents.to(torch_device)\n",
        "latents = latents * scheduler.sigmas[0] # Need to scale to match k\n",
        "\n",
        "# Loop\n",
        "with autocast(\"cuda\"):\n",
        "  for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
        "    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
        "\n",
        "    im_input = latents_to_pil(latents)[0]\n",
        "\n",
        "    latent_model_input = torch.cat([latents] * 2)\n",
        "    sigma = scheduler.sigmas[i]\n",
        "    latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
        "\n",
        "    # predict the noise residual\n",
        "    with torch.no_grad():\n",
        "      noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings)[\"sample\"]\n",
        "\n",
        "    # perform guidance\n",
        "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "    # Get the predicted x0:\n",
        "    latents_x0 = latents - sigma * noise_pred\n",
        "\n",
        "    im_t0 = latents_to_pil(latents_x0)[0]\n",
        "    im_noise = latents_to_pil(sigma * noise_pred)[0]\n",
        "\n",
        "    # And the previous noisy sample x_t -> x_t-1\n",
        "    latents = scheduler.step(noise_pred, t, latents)[\"prev_sample\"]\n",
        "    im_next = latents_to_pil(latents)[0]\n",
        "\n",
        "    # Combine the two images and save for later viewing\n",
        "    im = Image.new('RGB', (2304, 512))\n",
        "    im.paste(im_input, (0, 0))\n",
        "    im.paste(im_noise, (768, 0))\n",
        "    im.paste(im_t0, (1536, 0))\n",
        "    im.save(f'/content/drive/MyDrive/stablediff/denoising2/{i:04}.jpg')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}